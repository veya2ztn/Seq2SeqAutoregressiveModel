{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1048cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.custom_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd1d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.patch_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f91bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 2D patch model, the img_size will be force set (14, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "model = POverLapTimePosVallinaViT((14,32,64),(3,5,5),in_chans=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42984e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.pretrain import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e369b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args(config_path=\"checkpoints/WeathBench7066PatchDataset/ViT_in_bulk-POverLapTimePosVallinaViT_Patch_(3, 5, 5)/time_step_2_pretrain-3D70N_every_1_step_random_dataset/12_07_22_50_36-seed_2018/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac20a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use offline data mode <1>: only train use offline data\n",
      "use dataset in datasets/weatherbench_6hour\n",
      "load data from datasets/weatherbench_6hour/test.npy\n",
      "notice we will use around_index(12, 28, 64, 3, 3, 5, 5) to patch data\n"
     ]
    }
   ],
   "source": [
    "time_step = args.time_step if \"fourcast\" in args.mode else 3*24//6 + args.time_step\n",
    "dataset_kargs = copy.deepcopy(args.dataset_kargs)\n",
    "dataset_kargs['time_step'] = time_step\n",
    "if dataset_kargs['time_reverse_flag'] in ['only_forward','random_forward_backward']:\n",
    "    dataset_kargs['time_reverse_flag'] = 'only_forward'\n",
    "dataset_type = eval(args.dataset_type) if isinstance(args.dataset_type,str) else args.dataset_type\n",
    "#print(dataset_kargs)\n",
    "split = args.split if hasattr(args,'split') and args.split else \"test\"\n",
    "test_dataset = dataset_type(split=split, with_idx=True,dataset_tensor=None,record_load_tensor=None,**dataset_kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "305f4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.cross_sample = False\n",
    "idx_a,batch = test_dataset[0]\n",
    "tensor_a, time_stamp_a, pos_a = batch[0]\n",
    "input_a = model.image_to_patches(tensor_a[None],pos_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d140a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tensor_a[None]\n",
    "good_input_shape = model.patch_range\n",
    "now_input_shape  = tuple(x.shape[-3:])\n",
    "\n",
    "around_index = model.around_index_depend_input(now_input_shape)\n",
    "if model.global_position_feature is None:\n",
    "    grid = torch.Tensor(np.stack(np.meshgrid(\n",
    "            np.arange(model.img_size[0]),\n",
    "            np.arange(model.img_size[1]),\n",
    "            np.arange(model.img_size[2]))).transpose(0,2,1,3))\n",
    "    model.global_position_feature = model.get_direction_from_stamp(grid[None]) #(1, 3, Z,W, H)\n",
    "pos_feature  = model.global_position_feature.repeat(x.size(0),1,1,1,1).to(x.device) #(B, 3, Z,W, H)\n",
    "B,P,_,_,_ = x.shape\n",
    "x = torch.cat([x,pos_feature],1) #( B, 77, W, H)\n",
    "# x = x[...,around_index[:,:,:,0],around_index[:,:,:,1],around_index[:,:,:,2]] # (B,P,Z,W-4,H,Patch,Patch)\n",
    "# B,_,Z,W,H,_,_,_ = x.shape\n",
    "# model.input_shape_tmp=(B,Z,W,H,P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6097a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 5, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f419a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c8ae98b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[10 10 10 10 10]\n",
      "   [10 10 10 10 10]\n",
      "   [10 10 10 10 10]\n",
      "   [10 10 10 10 10]\n",
      "   [10 10 10 10 10]]\n",
      "\n",
      "  [[11 11 11 11 11]\n",
      "   [11 11 11 11 11]\n",
      "   [11 11 11 11 11]\n",
      "   [11 11 11 11 11]\n",
      "   [11 11 11 11 11]]\n",
      "\n",
      "  [[12 12 12 12 12]\n",
      "   [12 12 12 12 12]\n",
      "   [12 12 12 12 12]\n",
      "   [12 12 12 12 12]\n",
      "   [12 12 12 12 12]]]\n",
      "\n",
      "\n",
      " [[[19 19 19 19 19]\n",
      "   [20 20 20 20 20]\n",
      "   [21 21 21 21 21]\n",
      "   [22 22 22 22 22]\n",
      "   [23 23 23 23 23]]\n",
      "\n",
      "  [[19 19 19 19 19]\n",
      "   [20 20 20 20 20]\n",
      "   [21 21 21 21 21]\n",
      "   [22 22 22 22 22]\n",
      "   [23 23 23 23 23]]\n",
      "\n",
      "  [[19 19 19 19 19]\n",
      "   [20 20 20 20 20]\n",
      "   [21 21 21 21 21]\n",
      "   [22 22 22 22 22]\n",
      "   [23 23 23 23 23]]]\n",
      "\n",
      "\n",
      " [[[18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]]\n",
      "\n",
      "  [[18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]]\n",
      "\n",
      "  [[18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]\n",
      "   [18 19 20 21 22]]]]\n"
     ]
    }
   ],
   "source": [
    "test_dataset.cross_sample = True\n",
    "idx_b,batch = test_dataset[0]\n",
    "(tensor_b, time_stamp_b, pos_b) = batch[0]\n",
    "input_b = model.image_to_patches(tensor_b[None],torch.Tensor(pos_b[None]).to(tensor_b.device))\n",
    "print(pos_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26d11170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_tensor= x[...,pos_b[0],pos_b[1],pos_b[2]]\n",
    "torch.dist(selected_tensor,input_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa99d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = torch.Tensor(np.stack(np.meshgrid(\n",
    "                np.arange(model.img_size[0]),\n",
    "                np.arange(model.img_size[1]),\n",
    "                np.arange(model.img_size[2]))).transpose(0,2,1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "207911f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "image_to_patches() missing 1 required positional argument: 'pos_stamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_to_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m c \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpatches_to_image(y)\n",
      "\u001b[0;31mTypeError\u001b[0m: image_to_patches() missing 1 required positional argument: 'pos_stamp'"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,5,14,32,64)\n",
    "y = model.image_to_patches(a)\n",
    "c = model.patches_to_image(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9cd01d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 14, 32, 64])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f5a3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1,70,3,5,5).cuda()\n",
    "p = torch.randn(1, 3,3,5,5).cuda()\n",
    "t = torch.randn(1,4).cuda()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b7a3f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 70, 3, 5, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a,t,p).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f07248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "513c0480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 28, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(np.stack(np.meshgrid(\n",
    "            np.arange(img_size[0]),\n",
    "            np.arange(img_size[1]),\n",
    "        )).transpose(0,2,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2ecb91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 28, 32, 36])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(np.stack(np.meshgrid(\n",
    "    np.arange(self.img_size[0]),\n",
    "    np.arange(self.img_size[1]),\n",
    "    np.arange(self.img_size[2]))).transpose(0,2,1,3)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa71757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 70, 14, 32, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a162291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*16*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "96a17ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torach.randn(1,2,4,4)\n",
    "m = torch.randint(2,(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0f6e64dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0000, -1.0000,  1.2643,  0.2839],\n",
       "          [-1.0000,  0.0204, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -0.7041],\n",
       "          [ 0.1266, -1.0000, -1.0000, -1.0000]],\n",
       "\n",
       "         [[-1.0000, -1.0000, -2.2420, -0.1599],\n",
       "          [-1.0000,  1.4979, -1.0000, -1.0000],\n",
       "          [-1.0000, -1.0000, -1.0000, -0.3242],\n",
       "          [ 0.3754, -1.0000, -1.0000, -1.0000]]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.masked_fill(m,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638e6af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#x = x.reshape(B,W,H,P,patch_range,patch_range)\n",
    "x = torch.randn(B,W,H,P,patch_range,patch_range)\n",
    "def get_x1(x,counting_matrix):\n",
    "    w_idx   = np.arange(0,L)\n",
    "    wes  = np.stack([w_idx, w_idx+1,w_idx+2, w_idx-1, w_idx-2],1)%L\n",
    "    x_idx   = np.arange(H)\n",
    "    xes = np.stack([x_idx, x_idx+1,x_idx+2, x_idx-1, x_idx-2],1)%H\n",
    "    yes = np.array([[2,  1,  0,  3,  4]])\n",
    "    x = torch.nn.functional.pad(x,(0,0, 0,0, 0,0, 0,0, 2 , 2 )) # only extend W\n",
    "    x     = x[:, wes, :,:,yes,:].sum(1) #(B, W, H, P, PS,PS) --> (W, B, H, P, PS)\n",
    "    x     = x[:, :, xes,:,yes].sum(1)  #(W, B, H, P, PS) --> (H, W, B, P)   \n",
    "    x     = x.permute(2,3,1,0)#(B,P, W,H)   \n",
    "    counting_matrix =counting_matrix.to(x.device)\n",
    "    x     = x/counting_matrix #(B,P, W,H)  / (1,1 , W,H)\n",
    "    return x\n",
    "def get_x2(x,counting_matrix):\n",
    "    w_idx   = np.arange(0,L)\n",
    "    wes  = np.stack([w_idx+2, w_idx+1, w_idx, w_idx-1, w_idx-2],1)%L\n",
    "    x_idx   = np.arange(H)\n",
    "    xes = np.stack([x_idx+2, x_idx+1, x_idx, x_idx-1, x_idx-2],1)%H\n",
    "    yes = np.array([[0 ,1, 2, 3, 4]])\n",
    "    x = torch.nn.functional.pad(x,(0,0, 0,0, 0,0, 0,0, 2 , 2 )) # only extend W\n",
    "    x     = x[:, wes, :,:,yes,:].sum(1) #(B, W, H, P, PS,PS) --> (W, B, H, P, PS)\n",
    "    x     = x[:, :, xes,:,yes].sum(1)  #(W, B, H, P, PS) --> (H, W, B, P)   \n",
    "    x     = x.permute(2,3,1,0)#(B,P, W,H)   \n",
    "    counting_matrix =counting_matrix.to(x.device)\n",
    "    x     = x/counting_matrix #(B,P, W,H)  / (1,1 , W,H)\n",
    "    return x\n",
    "print(torch.dist(get_x1(x,counting_matrix),get_x2(x,counting_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd2f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch.vit import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4decc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltool.universal_model_util import get_model_para_num\n",
    "from utils.tools import getModelSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d656d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.othermodels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c032af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CK_SWDformer_0505(BaseModel):\n",
    "        def __init__(self,*args,**kargs):\n",
    "            super().__init__()\n",
    "            print(\"this is pre-set model, we disable all config\")\n",
    "            self.backbone = SWD_former(patch_size= [1, 1],\n",
    "                                in_chans= 70,\n",
    "                                out_chans= 70,\n",
    "                                embed_dim= 768,\n",
    "                                window_size= (5,5),\n",
    "                                depths= [4, 4, 4],\n",
    "                                num_heads= [6, 6, 6],\n",
    "                                Weather_T=1,only_swin=True)\n",
    "        def forward(self,x):\n",
    "            return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e510c79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CK_SWDformer_0505()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c13e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.randn(1,70,5,5)\n",
    "model(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2376dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "W=32\n",
    "H=64\n",
    "hd=16\n",
    "D=768\n",
    "B=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74121e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.SWD_former import SWD_former,SWDfromer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe4b125",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "model = SWD_former(patch_size= [1, 1],\n",
    "        in_chans= 70,\n",
    "        out_chans= 70,\n",
    "        embed_dim= 1128,\n",
    "        window_size= (32,64),\n",
    "        depths= [4, 4, 4],\n",
    "        num_heads= [6, 6, 6],\n",
    "        Weather_T=1,only_swin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a9e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltool.universal_model_util import get_model_para_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079cc9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.afnonet import AFNONet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cca764",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_para_num(model)[1]/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fde186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cephdataset import WeathBench7066PatchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919fe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(1,70,32,64)\n",
    "x = model.backbone.to_patch_embedding(img);print(x.shape)\n",
    "b, n, _ = x.shape\n",
    "\n",
    "cls_tokens = repeat(model.backbone.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "x = torch.cat((cls_tokens, x), dim=1);print(x.shape)\n",
    "x += model.backbone.pos_embedding[:, :(n + 1)];print(x.shape)\n",
    "x = model.backbone.dropout(x);print(x.shape)\n",
    "\n",
    "x = model.backbone.transformer(x);print(x.shape)\n",
    "\n",
    "x = x.mean(dim = 1) if model.backbone.pool == 'mean' else x[:, 0];print(x.shape)\n",
    "\n",
    "x = model.backbone.to_latent(x);print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_names = test_dataset.vnames\n",
    "accu_list = torch.stack([p['accu'] for p in fourcastresult.values() if 'accu' in p]).numpy()\n",
    "total_num = len(accu_list)\n",
    "accu_list = accu_list.mean(0)# (fourcast_num,property_num)\n",
    "real_times = [(predict_time+1)*test_dataset.time_intervel*test_dataset.time_unit for predict_time in range(len(accu_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa91ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_tables=[]\n",
    "snap_index = fourcastresult['snap_index']\n",
    "select_snap_start_time_point = snap_index[0]\n",
    "select_snap_show_property_id = snap_index[1]\n",
    "select_snap_property_name    = [property_names[iidd] for iidd in select_snap_show_property_id]\n",
    "for select_time_point in select_snap_start_time_point:\n",
    "    timestamp = test_dataset.datatimelist_pool['test'][select_time_point]\n",
    "    if select_time_point in fourcastresult: # in case do not record\n",
    "        linedata = fourcastresult[select_time_point]['snap_line']\n",
    "        for predict_time_point, tensor, label in linedata:\n",
    "            for name, value in zip(select_snap_property_name,tensor):\n",
    "                predict_timestamp = 0 if predict_time_point==0 else real_times[predict_time_point-1]\n",
    "                snap_tables.append([timestamp, name, predict_timestamp, value.item(), label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80baabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_rmse_map = fourcastresult['global_rmse_map']\n",
    "mean_global_rmse_map = [t/total_num for t in global_rmse_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c1aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b7a7c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236442b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!pip install empatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab525d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from empatches import EMPatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b23f2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emp = EMPatches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654095e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img_patches, indices = emp.extract_patches(img, patchsize=512, overlap=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6beb31",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "line.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a598a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "yes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c0e2d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(topvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8dde77",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(64):\n",
    "    [i,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e136c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "patch_x_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1667b",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b252da",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(23,70,32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebb9f9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_idx  = np.array([1,2])\n",
    "proper_idx = np.array([3,4]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a797a11",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a[0,0][2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec87ad12",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a[0,0][[2,3,4],[1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f055b38",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a.std(dim=(1,2,3)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b64e9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = torch.tensor([[1, 2], [3, 4]])\n",
    "torch.gather(t, 1, torch.tensor([[0], [1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9651e7b8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pos = [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea540e27",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = np.random.randn(1,2,3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71032b86",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a[*pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65329308",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5308dd8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = np.random.randn(1500,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e55a2b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a.nbytes//8//1024//1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c243be",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.patch_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61917ed7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.afnonet import AFNONet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f0c28",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec19a790",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mltool.universal_model_util import get_model_para_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e26396",
   "metadata": {
    "code_folding": [
     22
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LargeMLP_3D(nn.Module):\n",
    "    '''\n",
    "    input is (B, P, patch_range_1,patch_range_2,patch_range_3)\n",
    "    output is (B,P)\n",
    "    ''' \n",
    "    def __init__(self,img_size=None,patch_range=5,in_chans=20, out_chans=20,p=0.1,**kargs):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_range = 5\n",
    "        if self.patch_range == 5:\n",
    "            cl = [5*5*5*in_chans,5*5*5*10,5*5*5*20,\n",
    "                  5*5*5*30,5*5*5*30,5*5*5*20,\n",
    "                  5*5*5*10,5*5*5*1,out_chans]\n",
    "            nnlist = []\n",
    "            for i in range(len(cl)-2):\n",
    "                nnlist+=[nn.Linear(cl[i],cl[i+1]),nn.Dropout(p=p),nn.Tanh()]\n",
    "            nnlist+=[nn.Linear(cl[-2],cl[-1])]\n",
    "            self.backbone = nn.Sequential(*nnlist)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.center_index,self.around_index=get_center_around_indexes_3D(self.patch_range,self.img_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        The input either (B,P,patch_range,patch_range) or (B,P,w,h)\n",
    "        The output then is  (B,P) or (B,P,w-patch_range//2,h-patch_range//2)\n",
    "        ''' \n",
    "        assert len(x.shape) == 5 #(B,P,Z,W,H)\n",
    "        input_is_full_image = False\n",
    "        if x.shape[-3:] == self.img_size:\n",
    "            input_is_full_image = True\n",
    "            x = x[..., self.around_index[:, :, : , 0],self.around_index[:, :, : , 1],self.around_index[:, :, : , 2]] \n",
    "            # (B,P,Z-2,W-2,H,Patch,Patch,Patch)\n",
    "            x = x.permute(0, 2, 3, 4, 1, 5, 6, 7)\n",
    "            B, Z, W, H, P, _, _, _ = x.shape\n",
    "            x = x.flatten(0, 3)  # (B* Z-2 * W-2 * H, Property, Patch,Patch,Patch)\n",
    "        assert tuple(x.shape[-3:]) == (self.patch_range,self.patch_range,self.patch_range)\n",
    "        x = self.backbone(x.flatten(-4,-1)) # (B* W-4 * H,P)\n",
    "        if input_is_full_image:\n",
    "            x = x.reshape(B, Z, W, H, P).permute(0, 4, 1, 2,3) #(B, Z-2,W-2,H,P)  -> (B,P, Z-2,W-2,H)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c4653",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = LargeMLP_3D((14,32,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96424c93",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.tools import getModelSize\n",
    "param_sum, buffer_sum, all_size = getModelSize(model)\n",
    "print(f\" Number of Parameters: {param_sum}, Number of Buffers: {buffer_sum}, Size of Model: {all_size:.4f} MB\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074816d1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.patch_model import NaiveConvModel2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad10412",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533b372",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = NaiveConvModel2D((32,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a1f9f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(1,20,32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c093c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.tools import get_center_around_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389817cf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img_shape=(32,64)\n",
    "patch_range=5\n",
    "center_index,around_index=get_center_around_indexes(patch_range,img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dad203",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "around_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20928164",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "patch_range=5\n",
    "delta = [list(range(-(patch_range//2),patch_range//2+1))]*2\n",
    "delta = np.meshgrid(*delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92177012",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250fd2da",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc0f40",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "delta = [list(range(-(patch_range//2),patch_range//2+1))]*len(center)\n",
    "delta = np.meshgrid(*delta)\n",
    "pos  = [c+dc for c,dc in zip(center,delta)]\n",
    "pos[-1]= pos[-1]%64\n",
    "\n",
    "px = x + dx\n",
    "py = y + dy\n",
    "np.stack([px.flatten(),py.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d72e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeda73b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "coordinate = np.arange(36).reshape(1,1,6,6)\n",
    "print(coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f245c1a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "coordinate[:,:,pos].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71673fe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(coordinate[:,:,x,y][0,0])\n",
    "print(coordinate[:,:,pos[0],pos[1]][0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7465c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.FEDformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66615d8e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nn.BatchNorm3d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f42098",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class moving_avg_spacetime(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        assert len(kernel_size)==3\n",
    "        self.kernel_size = np.array(kernel_size)\n",
    "        self.avg         = nn.AvgPool3d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "        self.pad_front   = self.kernel_size - 1-np.floor((self.kernel_size - 1) // 2)\n",
    "        self.pad_end     = np.floor((self.kernel_size - 1) // 2)\n",
    "        self.pad         = np.stack([self.pad_front,self.pad_end],1)[::-1].flatten().astype('int').tolist()\n",
    "        print(self.pad)\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        # the input must be (B,*Space,T,C)ï¼Œ the -1 dim is embed channel, the -2 dim is time channel\n",
    "        shape = x.shape\n",
    "        BSpace_shape = shape[:-2]\n",
    "        C = shape[-1]\n",
    "        permute_order = [0,-1] + list(range(1,len(shape)-1))\n",
    "        x = x.permute(*permute_order)#-->(B, *Space,T, C)-->(B, C, *Space,T)\n",
    "        x = self.avg(F.pad(x,self.pad, mode='replicate'))\n",
    "        permute_order = [0] + list(range(2,len(shape))) + [1]\n",
    "        x = x.permute(*permute_order)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2193e9f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layer = moving_avg_spacetime((5,3,2),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937ac27",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a=torch.randn(1,32,64,6,7)\n",
    "layer(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cafd9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.physics_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35d29e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.afnonet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245af06a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from utils.params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c0eae",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "args=get_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a877f498",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "backbone = AFNONet((32,64),2,265,20,depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a6933",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layer = DirectSpace_Feature_Model(args,backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b4558",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a= torch.randn(1,4,5,32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285eced2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.std_mean(layer(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b2ab5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layer = Second_Derivative_Layer()\n",
    "\n",
    "a=torch.randn(1,1,32,64)\n",
    "\n",
    "layer(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c6052",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "B=2\n",
    "P=4\n",
    "a=torch.randn(B,P,3,32,64).cuda()\n",
    "layer=  First_Derivative_Layer(dim=3).cuda()\n",
    "runtime_weight=layer.runtime_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a49e635",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = torch.conv3d(a.flatten(0,1).unsqueeze(1),runtime_weight).reshape(*a.shape[:-1],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a2493",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x2 = torch.conv1d(a.flatten(0,-2).unsqueeze(1),runtime_weight[0,0]).reshape(*a.shape[:-1],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db951f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.dist(x,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120e039",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.scatter(x,y)\n",
    "#plt.scatter([10],[10],'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42446069",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_fourier_drive(x, position,pad):\n",
    "\n",
    "    if len(position)==1:position = position[0]\n",
    "    base_len          = np.array(x.shape[position])\n",
    "    interpolate_shape = base_len + (base_len-1)*pad\n",
    "    interpolate_shape = tuple(interpolate_shape)\n",
    "    if isinstance(position,int) or len(position) == 1\n",
    "        x = x.transpose(-1,position)\n",
    "        out_shape = x.shape[:-1]\n",
    "        x = x.reshape(x.size(0),-1,x.size(-1))\n",
    "        b = torch.nn.functional.interpolate(x,interpolate_shape,mode='linear',align_corners=True)\n",
    "        bf = torch.fft.rfft(b,dim=-1)\n",
    "        kbf= bf*torch.fft.rfftfreq(b.size(-1)).reshape(1,1,-1)\n",
    "        kb = torch.fft.irfft(kbf,dim=-1)\n",
    "        index = torch.arange(0,b.size(-1)+1,base_len-1)\n",
    "        kb = kb[...,index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209174f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#from tkinter.messagebox import NO\n",
    "import numpy as np\n",
    "import torch,os,io,socket\n",
    "from torchvision import datasets, transforms\n",
    "hostname = socket.gethostname()\n",
    "from functools import lru_cache\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from utils.timefeatures import time_features\n",
    "import os\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efc1f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "datatimelist  = np.arange(np.datetime64(\"1979-01-01\"), np.datetime64(\"2016-01-01\"), np.timedelta64(6, \"h\"))\n",
    "timestamp = time_features(pd.to_datetime(datatimelist)).transpose(1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d452ab7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(np.datetime64(\"1980-12-31\")).dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb2999b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(datatimelist)[0].dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc284a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "timestamp[:10,[0,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b05b6f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from JCmodels.fourcastnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59fc89",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = AFNONet((32,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ac0b9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(1,4,5,32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c828a24",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model(a).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ae080",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### FEDformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55908ea1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### complex version AdaptiveFourierNeuralOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7331c5ac",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f64f13",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def multiply(input, weights):\n",
    "    return torch.einsum('...bd,bdk->...bk', input, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf839a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "B=1\n",
    "h=32;w=64;C=2\n",
    "num_blocks=1\n",
    "block_size=2\n",
    "\n",
    "w1 = torch.randn(num_blocks, block_size, block_size, dtype=torch.cfloat)\n",
    "b1 = torch.randn(num_blocks, block_size, dtype=torch.cfloat)\n",
    "w2 = torch.randn(num_blocks, block_size, block_size, dtype=torch.cfloat)\n",
    "b2 = torch.randn(num_blocks, block_size, dtype=torch.cfloat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa8d54",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def original_realize(x,w1,b1,w2,b2):\n",
    "    x = torch.fft.rfft2(x, dim=(1, 2), norm='ortho');\n",
    "    x = x.reshape(B, x.shape[1], x.shape[2], num_blocks, block_size)\n",
    "    x_real = F.relu(multiply(x.real, w1.real) - multiply(x.imag, w1.imag) + b1.real, inplace=True)\n",
    "    x_imag = F.relu(multiply(x.real, w1.imag) + multiply(x.imag, w1.real) + b1.imag, inplace=True)\n",
    "    x = torch.stack([x_real, x_imag], dim=-1)\n",
    "    x = torch.view_as_complex(x)\n",
    "    return x\n",
    "\n",
    "def complex_version_realize(x,w1,b1,w2,b2):\n",
    "    x = torch.fft.rfft2(x, dim=(1, 2), norm='ortho');\n",
    "    x = x.reshape(B, x.shape[1], x.shape[2], num_blocks, block_size)\n",
    "    x = multiply(x,w1)+b1\n",
    "    x = nonlinear_activate(x)\n",
    "    return x\n",
    "x  = torch.randn(B, h, w, C)\n",
    "y1 = original_realize(x,w1,b1,w2,b2)\n",
    "y2 = complex_version_realize(x,w1,b1,w2,b2)\n",
    "torch.dist(y1,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f8cb3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.afnonet import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ccd53",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### AFNONet3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d11df",
   "metadata": {
    "code_folding": [
     0,
     1,
     6,
     70,
     103,
     132,
     185,
     192,
     203,
     206,
     213,
     223,
     226,
     242
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PartReLU_Complex(nn.Module):\n",
    "    def forward(self,x):\n",
    "        F.relu(x.real, inplace=True)\n",
    "        F.relu(x.imag, inplace=True)\n",
    "        return x\n",
    "\n",
    "class AdaptiveFourierNeuralOperator(nn.Module):\n",
    "    def __init__(self, dim, img_size, fno_blocks=4,fno_bias=True, fno_softshrink=False,nonlinear_activate=PartReLU_Complex()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = dim\n",
    "        self.img_size   = img_size\n",
    "        self.num_blocks = fno_blocks\n",
    "        self.block_size = self.hidden_size // self.num_blocks\n",
    "        assert self.hidden_size % self.num_blocks == 0\n",
    "\n",
    "        self.scale = 0.02\n",
    "        self.w1 = torch.nn.Parameter(self.scale * torch.randn(self.num_blocks, self.block_size, \n",
    "                                                              self.block_size, dtype=torch.cfloat))\n",
    "        self.b1 = torch.nn.Parameter(self.scale * torch.randn(self.num_blocks, self.block_size, \n",
    "                                                              dtype=torch.cfloat))\n",
    "        self.w2 = torch.nn.Parameter(self.scale * torch.randn(self.num_blocks, self.block_size, \n",
    "                                                              self.block_size, dtype=torch.cfloat))\n",
    "        self.b2 = torch.nn.Parameter(self.scale * torch.randn(self.num_blocks, self.block_size, \n",
    "                                                              dtype=torch.cfloat))\n",
    "        self.relu = nonlinear_activate\n",
    "\n",
    "        if fno_bias:\n",
    "            self.bias = nn.Conv1d(self.hidden_size, self.hidden_size, 1)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.softshrink = fno_softshrink\n",
    "\n",
    "    def multiply(self, input, weights):\n",
    "        return torch.einsum('...bd,bdk->...bk', input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape    \n",
    "        bias = self.bias(x.permute(0, 2, 1)).permute(0, 2, 1) if self.bias else 0\n",
    "        #timer.restart(2)\n",
    "        x = x.reshape(B, *self.img_size, C)\n",
    "        fft_dim = tuple(range(1,len(self.img_size)+1))\n",
    "        print(fft_dim)\n",
    "        #timer.record('reshape1','filter',2)\n",
    "        print(x.shape)\n",
    "        x = torch.fft.rfftn(x, dim=fft_dim, norm='ortho');\n",
    "        #timer.record('rfft2','filter',2)\n",
    "        print(x.shape)\n",
    "        x = x.reshape(*x.shape[:-1], self.num_blocks, self.block_size)\n",
    "        #timer.record('reshape2','filter',2)\n",
    "        x = self.multiply(x,self.w1)+self.b1\n",
    "        x = self.relu(x)\n",
    "        x = self.multiply(x,self.w2)+self.b2\n",
    "        x = F.softshrink(x, lambd=self.softshrink) if self.softshrink else x\n",
    "        #with torch.cuda.amp.autocast(enabled=False):\n",
    "        #x = x.float()   \n",
    "        #x = torch.view_as_complex(x)\n",
    "        #timer.record('reset','filter',2)\n",
    "        x = x.flatten(-2,-1)\n",
    "        #timer.record('reshape3','filter',2)\n",
    "        print(x.shape)\n",
    "        x = torch.fft.irfftn(x, s=self.img_size,dim=fft_dim, norm='ortho')\n",
    "        print(x.shape)\n",
    "        #x = x.half()\n",
    "        #timer.record('irfft2','filter',2)\n",
    "        x = x.reshape(B, N, C)\n",
    "        #timer.record('reshape4','filter',2)\n",
    "        return x + bias\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio=4., drop=0., drop_path=0., act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm, region_shape=(14,8), fno_blocks=3,double_skip=False, fno_bias=False, fno_softshrink=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.filter = AdaptiveFourierNeuralOperator(dim, region_shape,fno_blocks=fno_blocks,fno_bias=fno_bias,fno_softshrink=fno_softshrink)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.double_skip = double_skip\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        #timer.restart(1)\n",
    "        x = self.norm1(x)\n",
    "        #timer.record('norm1','forward_features',1)\n",
    "        x = self.filter(x)\n",
    "        #timer.record('filter','forward_features',1)\n",
    "        if self.double_skip:\n",
    "            x += residual\n",
    "            residual = x;\n",
    "        #timer.record('residual','forward_features',1)\n",
    "        x = self.norm2(x)\n",
    "        #timer.record('norm2','forward_features',1)\n",
    "        x = self.mlp(x)\n",
    "        #timer.record('mlp','forward_features',1)\n",
    "        x = self.drop_path(x)\n",
    "        #timer.record('drop_path','forward_features',1)\n",
    "        x += residual\n",
    "        #timer.record('residual','forward_features',1)\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=None, patch_size=8, in_chans=13, embed_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        if img_size is None:raise KeyError('img is None')\n",
    "        patch_size   = [patch_size]*len(img_size) if isinstance(patch_size,int) else patch_size\n",
    "        \n",
    "        num_patches=1\n",
    "        out_size=[]\n",
    "        for i_size,p_size in zip(img_size,patch_size):\n",
    "            if p_size%i_size:\n",
    "                num_patches*=i_size// p_size\n",
    "                out_size.append(i_size// p_size)\n",
    "            else:\n",
    "                raise NotImplementedError(f\"the patch size ({patch_size}) cannot divide the img size {img_size}\")\n",
    "        self.img_size    = tuple(img_size)\n",
    "        self.patch_size  = tuple(patch_size)\n",
    "        self.num_patches = num_patches\n",
    "        self.out_size    = tuple(out_size)\n",
    "        conv_engine = [nn.Conv1d,nn.Conv2d,nn.Conv3d]\n",
    "        self.proj   = conv_engine[len(img_size)-1](in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, = x.shape[:2]\n",
    "        inp_size = x.shape[2:]\n",
    "        assert tuple(inp_size) == self.img_size, f\"Input image size ({inp_size}) doesn't match model set size ({self.img_size}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class AFNONet(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size=8, in_chans=20, out_chans=20, embed_dim=768, depth=12, mlp_ratio=4.,\n",
    "                 uniform_drop=False, drop_rate=0., drop_path_rate=0., norm_layer=None,\n",
    "                 dropcls=0, checkpoint_activations=False, fno_blocks=3,double_skip=False,\n",
    "                 fno_bias=False, fno_softshrink=False,debug_mode=False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert img_size is not None\n",
    "        self.checkpoint_activations=checkpoint_activations\n",
    "        self.embed_dim = embed_dim\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        self.img_size = img_size\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches      = self.patch_embed.num_patches\n",
    "        patch_size       = self.patch_embed.patch_size\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        self.final_shape = self.patch_embed.out_size\n",
    "\n",
    "        if uniform_drop:\n",
    "            dpr = [drop_path_rate for _ in range(depth)]  # stochastic depth decay rule\n",
    "        else:\n",
    "            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "\n",
    "        self.blocks = nn.ModuleList([Block(dim=embed_dim, mlp_ratio=mlp_ratio, drop=drop_rate,\n",
    "                                           drop_path=dpr[i], \n",
    "                                           norm_layer=norm_layer,\n",
    "                                           region_shape=self.final_shape,\n",
    "                                           double_skip=double_skip,\n",
    "                                           fno_blocks=fno_blocks,\n",
    "                                           fno_bias=fno_bias,\n",
    "                                           fno_softshrink=fno_softshrink) for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        # self.num_features = out_chans * img_size[0] * img_size[1]\n",
    "        # self.representation_size = self.num_features * 8\n",
    "        # self.pre_logits = nn.Sequential(OrderedDict([\n",
    "        #     ('fc', nn.Linear(embed_dim, self.representation_size)),\n",
    "        #     ('act', nn.Tanh())\n",
    "        # ]))\n",
    "        conf_list = [{'kernel_size':[],'stride':[],'padding':[]},\n",
    "                     {'kernel_size':[],'stride':[],'padding':[]},\n",
    "                     {'kernel_size':[],'stride':[],'padding':[]}]\n",
    "        conv_set = {8:[[2,2,0],[2,2,0],[2,2,0]],\n",
    "                    4:[[2,2,0],[3,1,1],[2,2,0]],\n",
    "                    2:[[3,1,1],[3,1,1],[2,2,0]],\n",
    "                    1:[[3,1,1],[3,1,1],[3,1,1]],\n",
    "                   }\n",
    "        for patch in patch_size:\n",
    "            for slot in range(len(conf_list)):\n",
    "                conf_list[slot]['kernel_size'].append(conv_set[patch][slot][0])\n",
    "                conf_list[slot]['stride'].append(conv_set[patch][slot][1])\n",
    "                conf_list[slot]['padding'].append(conv_set[patch][slot][2])\n",
    "\n",
    "        transposeconv_engine = [nn.ConvTranspose1d,nn.ConvTranspose2d,nn.ConvTranspose3d][len(img_size)-1]\n",
    "        self.pre_logits = nn.Sequential(OrderedDict([\n",
    "            ('conv1', transposeconv_engine(embed_dim, out_chans*16, **conf_list[0])),\n",
    "            ('act1', nn.Tanh()),\n",
    "            ('conv2', transposeconv_engine(out_chans*16, out_chans*4, **conf_list[1])),\n",
    "            ('act2', nn.Tanh())\n",
    "        ]))\n",
    "\n",
    "        # Generator head\n",
    "        # self.head = nn.Linear(self.representation_size, self.num_features)\n",
    "        self.head = transposeconv_engine(out_chans*4, out_chans, **conf_list[2])\n",
    "\n",
    "        if dropcls > 0:\n",
    "            print('dropout %.2f before classifier' % dropcls)\n",
    "            self.final_dropout = nn.Dropout(p=dropcls)\n",
    "        else:\n",
    "            self.final_dropout = nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "        self.debug_mode=debug_mode\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        x += self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        if not self.checkpoint_activations:\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x)\n",
    "        else:\n",
    "            x = checkpoint_sequential(self.blocks, 4, x)\n",
    "\n",
    "        x = self.norm(x).transpose(1, 2);print(x.shape)\n",
    "        x = torch.reshape(x, [-1, self.embed_dim, *self.final_shape])\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### we assume always feed the tensor (B, p*z, h, w)\n",
    "        B, P ,h, w = x.shape\n",
    "        x = x.reshape(B,-1,*self.img_size)\n",
    "        #timer.restart(level=0)\n",
    "        x = self.forward_features(x)\n",
    "        #timer.record('forward_features',level=0)\n",
    "        x = self.final_dropout(x)\n",
    "        #timer.record('final_dropout',level=0)\n",
    "        x = self.pre_logits(x)\n",
    "        #timer.record('pre_logits',level=0)\n",
    "        x = self.head(x)\n",
    "        #timer.record('head',level=0)\n",
    "        x = x.reshape(B,P,h,w)\n",
    "        #timer.show_stat()\n",
    "        #print(\"============================\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c91758",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = AFNONet(img_size=[10,32,64],patch_size=(1,8,8),depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094685c7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(1,20,10,32,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f0892",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(1,3,4,32,64).flatten(1,2)\n",
    "torch.dist(a.reshape(1,-1,4,32,64).reshape(1,12,32,64),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc73c64",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(1,12,32,64)\n",
    "torch.dist(a.reshape(1,3,4,32,64).reshape(1,-1,32,64),a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e691946",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### EulerEquationModel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2f336",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EulerEquationModel2(nn.Module):\n",
    "    def __init__(self, args, backbone):\n",
    "        super().__init__()\n",
    "        self.Dx= First_Derivative_Layer(position=-1, dim=3, mode=2)\n",
    "        self.Dy= First_Derivative_Layer(position=-2, dim=3, mode=2)\n",
    "        self.Dz= First_Derivative_Layer(position=-3, dim=3, mode=1)\n",
    "        self.thermal_factor  = nn.Parameter(torch.randn(3).reshape(1,3,1,1))\n",
    "        self.alpha = nn.Parameter(torch.randn(1))\n",
    "        #self.p_list         = nn.Parameter(torch.Tensor([10,8.5,5]).reshape(1,3,1,1),requires_grad=False)\n",
    "        self.backbone =  backbone\n",
    "        self.monitor = True\n",
    "    def forward(self, Field):\n",
    "        # u^{t+1} &= u^{t} + F_x - \\nabla (Vu)  + u \\nabla\\cdot V - \\partial_x\\phi\\\\\n",
    "        # v^{t+1} &= v^{t} + F_y - \\nabla (Vv)  + v \\nabla\\cdot V - \\partial_y\\phi\\\\\n",
    "        # T^{t+1} &= T^{t} + Q/C_v + \\frac{RT}{C_pp}\\omega  - \\nabla (VT) +T \\nabla\\cdot V\\\\\n",
    "        # \\phi^{t+1}&=\\phi^{t} + wg  - \\nabla (V\\phi)+ \\phi \\nabla\\cdot V \\\\\n",
    "        # 0&\\approx \\nabla\\cdot V\n",
    "        # input -> Field  = [u ,v, T, p] --> (Batch, 4, z, y ,x)\n",
    "        # need generate unknown data [Fx, Fy , Q, W, o]\n",
    "        b, si_z, i_y, i_x = Field.shape\n",
    "        s=4\n",
    "        i_z= si_z//4\n",
    "        MachineLearningPart = self.backbone(Field).reshape(b, s+1, i_z, i_y, i_x) #(Batch, 5, z, y ,x)\n",
    "        ExternalForce = MachineLearningPart[:,:4] #(Batch, 4, z, y ,x)\n",
    "        o = MachineLearningPart[:,4:5] #(Batch, 1, z, y ,x)\n",
    "        Field = Field.reshape(b, s, i_z, i_y,  i_x) #(Batch, 5, z, y ,x)\n",
    "        u = Field[:,0:1]#(Batch, 1, z, y ,x)\n",
    "        v = Field[:,1:2]#(Batch, 1, z, y ,x)\n",
    "        T = Field[:,2:3]#(Batch, 1, z, y ,x)\n",
    "        p = Field[:,3:4]#(Batch, 1, z, y ,x)\n",
    "        V = torch.cat([u,v,o],1)#(Batch, 3, z, y ,x)\n",
    "        Nabla_cdot_V = (self.Dx(u[:,0]) + self.Dy(v[:,0]) + self.Dz(o[:,0])).unsqueeze(1)#(Batch, 1, z, y ,x)\n",
    "        Nabla_V_Field= Nabla_cdot_V*Field #(Batch, 4, z, y ,x)\n",
    "        Vphysics     = torch.stack([V*u,V*v,V*T,V*p],1)#(Batch,4, 3, z, y ,x)\n",
    "        Vphysics_dx = self.Dx(Vphysics[:,:,0].flatten(0,1)).reshape(Field.shape)#(Batch, 4, z, y ,x)\n",
    "        Vphysics_dy = self.Dy(Vphysics[:,:,1].flatten(0,1)).reshape(Field.shape)#(Batch, 4, z, y ,x)\n",
    "        Vphysics_dz = self.Dz(Vphysics[:,:,2].flatten(0,1)).reshape(Field.shape)#(Batch, 4, z, y ,x)\n",
    "        PhysicsPart = -Vphysics_dx - Vphysics_dy - Vphysics_dz + Nabla_V_Field #(Batch,4,z, y ,x)\n",
    "        InteractionPart= torch.stack([self.Dx(p[:,0]),\n",
    "                                      self.Dy(p[:,0]),\n",
    "                                      self.thermal_factor*T[:,0]*o[:,0]],1)#(Batch,3,z, y ,x)\n",
    "        InteractionPart= F.pad(InteractionPart,(0,0,0,0,0,0,0,1)) #(Batch,4,z, y ,x)\n",
    "        Delta_Fd     = ExternalForce + self.alpha*(PhysicsPart + InteractionPart)\n",
    "        Field        = Field+ Delta_Fd\n",
    "        constrain    = Nabla_V_Field\n",
    "        if not self.monitor:\n",
    "            return Field.flatten(1,2),(constrain**2).mean()\n",
    "        else:\n",
    "            return Field.flatten(1,2),(constrain**2).mean(),{\"ExternalForceFactor\":(ExternalForce**2).mean().item(),\n",
    "                                                             \"PhysicsDrivenFactor\":(PhysicsPart**2).mean().item(),\n",
    "                                                             \"InteractionPart\":(InteractionPart**2).mean().item(),\n",
    "                                                             \"alpha\":self.alpha.item()}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6b4e50d8356ba75a9636787f9051ee458aaf13e87df491415cc800c7b2b8a21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
